{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776290e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구축된 단어 사전 크기: 25\n",
      "Word2Vec 모델 로딩 중...\n",
      "모델 파일이 없어 임시 차원(100)으로 진행합니다.\n",
      "임베딩 매트릭스 생성 완료. OOV(사전에 없는 단어) 개수: 24\n",
      "DeepCoNN 임베딩 레이어 연동 완료!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from konlpy.tag import Okt\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. 형태소 분석 및 단어 사전(Vocabulary) 구축\n",
    "# ---------------------------------------------------------\n",
    "jvm_path = r'C:\\Program Files\\Java\\jdk-25.0.2\\bin\\server\\jvm.dll'\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-25.0.2'   # 꼭 추가\n",
    "\n",
    "okt = Okt()\n",
    "reviews = [\n",
    "    \"웨이팅이 너무 길었지만 존맛탱이네요! 가성비 최고.\",\n",
    "    \"직원들이 불친절하고 맛도 그냥 그랬어요. 재방문 의사 없음\"\n",
    "]\n",
    "\n",
    "# 토큰화 진행\n",
    "tokenized_reviews = [okt.morphs(review, stem=True) for review in reviews]\n",
    "\n",
    "# 데이터셋에 등장한 모든 단어를 모아 고유 인덱스 부여\n",
    "word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for tokens in tokenized_reviews:\n",
    "    for word in tokens:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"구축된 단어 사전 크기: {vocab_size}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. 사전 학습된 Word2Vec 로드 및 Embedding Matrix 생성\n",
    "# ---------------------------------------------------------\n",
    "# 주의: 사전에 학습된 ko.bin 또는 ko.model 파일이 다운로드 되어 있어야 합니다.\n",
    "# (예: Kyubyong의 한국어 Word2Vec 모델)\n",
    "print(\"Word2Vec 모델 로딩 중...\")\n",
    "try:\n",
    "    # gensim을 이용해 가벼운 모델 로드\n",
    "    word2vec_model = gensim.models.Word2Vec.load('') \n",
    "    embed_dim = word2vec_model.vector_size\n",
    "except FileNotFoundError:\n",
    "    print(\"모델 파일이 없어 임시 차원(100)으로 진행합니다.\")\n",
    "    word2vec_model = {} # 실제 환경에서는 다운로드한 모델이 들어갑니다.\n",
    "    embed_dim = 100\n",
    "\n",
    "# 모델의 가중치를 담을 빈 행렬 생성 (vocab_size x embed_dim)\n",
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "\n",
    "# 다이닝코드 단어 사전을 순회하며 Word2Vec에서 벡터값을 가져옴\n",
    "oov_count = 0\n",
    "for word, i in word_to_idx.items():\n",
    "    if word in word2vec_model:\n",
    "        # 사전 학습된 벡터가 있으면 가져옴\n",
    "        embedding_matrix[i] = word2vec_model[word]\n",
    "    else:\n",
    "        # 사전에 없는 단어(OOV)는 정규분포를 따르는 무작위 벡터로 초기화\n",
    "        # <PAD>는 0벡터로 유지\n",
    "        if word != \"<PAD>\":\n",
    "            embedding_matrix[i] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "            oov_count += 1\n",
    "\n",
    "print(f\"임베딩 매트릭스 생성 완료. OOV(사전에 없는 단어) 개수: {oov_count}\")\n",
    "embedding_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. PyTorch DeepCoNN 모델에 주입\n",
    "# ---------------------------------------------------------\n",
    "class DeepCoNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix_tensor, num_filters=100, kernel_size=3):\n",
    "        super(DeepCoNN, self).__init__()\n",
    "        \n",
    "        # from_pretrained를 사용하여 우리가 만든 행렬을 모델의 Embedding 층으로 만듦\n",
    "        # 식당 도메인에 맞게 학습되도록 freeze=False로 설정 (가중치 업데이트 허용)\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embedding_matrix_tensor, \n",
    "            freeze=False, \n",
    "            padding_idx=0\n",
    "        )\n",
    "        \n",
    "        embed_dim = embedding_matrix_tensor.shape[1]\n",
    "        \n",
    "        # User Network (CNN) 예시\n",
    "        self.user_cnn = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, num_filters, kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Seq_len)\n",
    "        embedded = self.embedding(x) # (Batch, Seq_len, Embed_dim)\n",
    "        embedded = embedded.permute(0, 2, 1) # CNN 입력을 위해 차원 변경: (Batch, Embed_dim, Seq_len)\n",
    "        out = self.user_cnn(embedded)\n",
    "        return out.squeeze(2)\n",
    "\n",
    "# 모델 생성 테스트\n",
    "model = DeepCoNN(embedding_matrix_tensor)\n",
    "print(\"DeepCoNN 임베딩 레이어 연동 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3965a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kyubyong Word2Vec 로딩 중...\n",
      "로딩 완료!\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# 압축을 푼 ko.bin 파일의 경로를 지정합니다.\n",
    "print(\"Kyubyong Word2Vec 로딩 중...\")\n",
    "ft_model = KeyedVectors.load_word2vec_format(\n",
    "    '../../datafile/word2vec/cc.ko.300.vec', \n",
    "    binary=False, \n",
    "    limit=50000)\n",
    "print(\"로딩 완료!\")\n",
    "\n",
    "# 단어 벡터 확인 예시\n",
    "vector = ft_model['맛집']\n",
    "print(vector.shape) # 보통 (200,) 차원을 가집니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
