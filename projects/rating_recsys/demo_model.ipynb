{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776290e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구축된 단어 사전 크기: 25\n",
      "Word2Vec 모델 로딩 중...\n",
      "모델 파일이 없어 임시 차원(100)으로 진행합니다.\n",
      "임베딩 매트릭스 생성 완료. OOV(사전에 없는 단어) 개수: 24\n",
      "DeepCoNN 임베딩 레이어 연동 완료!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from konlpy.tag import Okt\n",
    "import gensim\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. 형태소 분석 및 단어 사전(Vocabulary) 구축\n",
    "# ---------------------------------------------------------\n",
    "jvm_path = r'C:\\Program Files\\Java\\jdk-25.0.2\\bin\\server\\jvm.dll'\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-25.0.2'   # 꼭 추가\n",
    "\n",
    "okt = Okt()\n",
    "reviews = [\n",
    "    \"웨이팅이 너무 길었지만 존맛탱이네요! 가성비 최고.\",\n",
    "    \"직원들이 불친절하고 맛도 그냥 그랬어요. 재방문 의사 없음\"\n",
    "]\n",
    "\n",
    "# 토큰화 진행\n",
    "tokenized_reviews = [okt.morphs(review, stem=True) for review in reviews]\n",
    "\n",
    "# 데이터셋에 등장한 모든 단어를 모아 고유 인덱스 부여\n",
    "word_to_idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for tokens in tokenized_reviews:\n",
    "    for word in tokens:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "vocab_size = len(word_to_idx)\n",
    "print(f\"구축된 단어 사전 크기: {vocab_size}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. 사전 학습된 Word2Vec 로드 및 Embedding Matrix 생성\n",
    "# ---------------------------------------------------------\n",
    "# 주의: 사전에 학습된 ko.bin 또는 ko.model 파일이 다운로드 되어 있어야 합니다.\n",
    "# (예: Kyubyong의 한국어 Word2Vec 모델)\n",
    "print(\"Word2Vec 모델 로딩 중...\")\n",
    "try:\n",
    "    # gensim을 이용해 가벼운 모델 로드\n",
    "    word2vec_model = gensim.models.Word2Vec.load('') \n",
    "    embed_dim = word2vec_model.vector_size\n",
    "except FileNotFoundError:\n",
    "    print(\"모델 파일이 없어 임시 차원(100)으로 진행합니다.\")\n",
    "    word2vec_model = {} # 실제 환경에서는 다운로드한 모델이 들어갑니다.\n",
    "    embed_dim = 100\n",
    "\n",
    "# 모델의 가중치를 담을 빈 행렬 생성 (vocab_size x embed_dim)\n",
    "embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "\n",
    "# 다이닝코드 단어 사전을 순회하며 Word2Vec에서 벡터값을 가져옴\n",
    "oov_count = 0\n",
    "for word, i in word_to_idx.items():\n",
    "    if word in word2vec_model:\n",
    "        # 사전 학습된 벡터가 있으면 가져옴\n",
    "        embedding_matrix[i] = word2vec_model[word]\n",
    "    else:\n",
    "        # 사전에 없는 단어(OOV)는 정규분포를 따르는 무작위 벡터로 초기화\n",
    "        # <PAD>는 0벡터로 유지\n",
    "        if word != \"<PAD>\":\n",
    "            embedding_matrix[i] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
    "            oov_count += 1\n",
    "\n",
    "print(f\"임베딩 매트릭스 생성 완료. OOV(사전에 없는 단어) 개수: {oov_count}\")\n",
    "embedding_matrix_tensor = torch.FloatTensor(embedding_matrix)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. PyTorch DeepCoNN 모델에 주입\n",
    "# ---------------------------------------------------------\n",
    "class DeepCoNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix_tensor, num_filters=100, kernel_size=3):\n",
    "        super(DeepCoNN, self).__init__()\n",
    "        \n",
    "        # from_pretrained를 사용하여 우리가 만든 행렬을 모델의 Embedding 층으로 만듦\n",
    "        # 식당 도메인에 맞게 학습되도록 freeze=False로 설정 (가중치 업데이트 허용)\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embedding_matrix_tensor, \n",
    "            freeze=False, \n",
    "            padding_idx=0\n",
    "        )\n",
    "        \n",
    "        embed_dim = embedding_matrix_tensor.shape[1]\n",
    "        \n",
    "        # User Network (CNN) 예시\n",
    "        self.user_cnn = nn.Sequential(\n",
    "            nn.Conv1d(embed_dim, num_filters, kernel_size),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Seq_len)\n",
    "        embedded = self.embedding(x) # (Batch, Seq_len, Embed_dim)\n",
    "        embedded = embedded.permute(0, 2, 1) # CNN 입력을 위해 차원 변경: (Batch, Embed_dim, Seq_len)\n",
    "        out = self.user_cnn(embedded)\n",
    "        return out.squeeze(2)\n",
    "\n",
    "# 모델 생성 테스트\n",
    "model = DeepCoNN(embedding_matrix_tensor)\n",
    "print(\"DeepCoNN 임베딩 레이어 연동 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3965a236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kyubyong Word2Vec 로딩 중...\n",
      "로딩 완료!\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# 압축을 푼 ko.bin 파일의 경로를 지정합니다.\n",
    "print(\"Kyubyong Word2Vec 로딩 중...\")\n",
    "ft_model = KeyedVectors.load_word2vec_format(\n",
    "    \"C:/Users/hanan/Downloads/cc.ko.300.vec.gz\", \n",
    "    binary=False, \n",
    "    limit=50000)\n",
    "print(\"로딩 완료!\")\n",
    "\n",
    "# 단어 벡터 확인 예시\n",
    "vector = ft_model['맛집']\n",
    "print(vector.shape) # 보통 (200,) 차원을 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9bc61",
   "metadata": {},
   "source": [
    "# demo model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899fbca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "config = Config()\n",
    "set_seed(config.random_seed)\n",
    "\n",
    "data = pd.read_csv('diningcode_data_crawling_20260125_1542.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "data['user_id'] = le.fit_transform(data['user_name'])\n",
    "data['item_id'] = le.fit_transform(data['item_name'])\n",
    "\n",
    "data['user_rating'] = data['user_rating'].str.replace('점','').astype(float)\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f64b9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>item_name</th>\n",
       "      <th>item_area</th>\n",
       "      <th>item_avg_rating</th>\n",
       "      <th>item_spec_area</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_tot_avg_rating</th>\n",
       "      <th>user_tot_rating_num</th>\n",
       "      <th>user_tot_follow_num</th>\n",
       "      <th>user_rating</th>\n",
       "      <th>user_query</th>\n",
       "      <th>taste</th>\n",
       "      <th>price</th>\n",
       "      <th>service</th>\n",
       "      <th>menu</th>\n",
       "      <th>date</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>서령 본점</td>\n",
       "      <td>남대문</td>\n",
       "      <td>4.6</td>\n",
       "      <td>서울특별시 중구 남대문로5가 120  1층</td>\n",
       "      <td>\\r\\n오또잇\\r\\n</td>\n",
       "      <td>3.9</td>\n",
       "      <td>278</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>먹어본 평양냉면 중에 제일 맛있었습니다 다른 곳들은 좀 짜다는 느낌이 강했는데 서령...</td>\n",
       "      <td>맛: 좋음</td>\n",
       "      <td>가격: 보통</td>\n",
       "      <td>응대: 좋음</td>\n",
       "      <td>서령 순면, 항정 제육 반 접시(100g)</td>\n",
       "      <td>2025년 7월 31일</td>\n",
       "      <td>2826</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>서령 본점</td>\n",
       "      <td>남대문</td>\n",
       "      <td>4.6</td>\n",
       "      <td>서울특별시 중구 남대문로5가 120  1층</td>\n",
       "      <td>\\r\\n와퍼\\r\\n</td>\n",
       "      <td>4.1</td>\n",
       "      <td>494</td>\n",
       "      <td>27</td>\n",
       "      <td>5.0</td>\n",
       "      <td>서울에서 가장 맛있는 평양냉면집.\\r\\n깔끔하고 슴슴한데 이렇게 맛있을 수가 없다....</td>\n",
       "      <td>맛: 좋음</td>\n",
       "      <td>가격: 만족</td>\n",
       "      <td>응대: 좋음</td>\n",
       "      <td>서령 순면</td>\n",
       "      <td>2025년 8월 17일</td>\n",
       "      <td>2866</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>서령 본점</td>\n",
       "      <td>남대문</td>\n",
       "      <td>4.6</td>\n",
       "      <td>서울특별시 중구 남대문로5가 120  1층</td>\n",
       "      <td>\\r\\n어먹행먹\\r\\n</td>\n",
       "      <td>4.2</td>\n",
       "      <td>209</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>서령 순면(평양냉면) 슴슴한데 은은한 특유의 육수맛이 있어요. 여기는 서령초라는 걸...</td>\n",
       "      <td>맛: 좋음</td>\n",
       "      <td>가격: 만족</td>\n",
       "      <td>응대: 좋음</td>\n",
       "      <td>서령 순면, 항정 제육 반 접시(100g)</td>\n",
       "      <td>2025년 5월 1일</td>\n",
       "      <td>2735</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>서령 본점</td>\n",
       "      <td>남대문</td>\n",
       "      <td>4.6</td>\n",
       "      <td>서울특별시 중구 남대문로5가 120  1층</td>\n",
       "      <td>\\r\\n다코미식가\\r\\n먹죽귀\\r\\n</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1339</td>\n",
       "      <td>669</td>\n",
       "      <td>5.0</td>\n",
       "      <td>중구 시청역 남대문 근처에 위치한\\r\\n서령 다녀왔어요예전에 강화도에 있을 때에도 ...</td>\n",
       "      <td>맛: 좋음</td>\n",
       "      <td>가격: 만족</td>\n",
       "      <td>응대: 좋음</td>\n",
       "      <td>서령 순면</td>\n",
       "      <td>2025년 2월 1일</td>\n",
       "      <td>1470</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>서령 본점</td>\n",
       "      <td>남대문</td>\n",
       "      <td>4.6</td>\n",
       "      <td>서울특별시 중구 남대문로5가 120  1층</td>\n",
       "      <td>\\r\\n쿨제이\\r\\n</td>\n",
       "      <td>3.7</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>평냉계에서 많은 사람들한테 회자되는 '서령'을 갔다왔어요. 결론적으로 제 취향이 아...</td>\n",
       "      <td>맛: 보통</td>\n",
       "      <td>가격: 불만</td>\n",
       "      <td>응대: 보통</td>\n",
       "      <td>서령 순면, 소주, 접시만두 반 접시(3개), 항정 제육 반 접시(100g)</td>\n",
       "      <td>2025년 8월 13일</td>\n",
       "      <td>3440</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12622</th>\n",
       "      <td>12622</td>\n",
       "      <td>연교</td>\n",
       "      <td>연남동</td>\n",
       "      <td>4.4</td>\n",
       "      <td>서울특별시 마포구 연남동 383-95  1층</td>\n",
       "      <td>\\r\\nmatziplist\\r\\n</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1187</td>\n",
       "      <td>23</td>\n",
       "      <td>4.0</td>\n",
       "      <td>모임하기 좋은 개별적인 공간있어서 좋아보였다. 소룡퍼 맛은 좀 서운했는데 다른것들이...</td>\n",
       "      <td>맛: 좋음</td>\n",
       "      <td>가격: 만족</td>\n",
       "      <td>응대: 보통</td>\n",
       "      <td>샤오롱바오</td>\n",
       "      <td>2020년 3월 30일</td>\n",
       "      <td>691</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12623</th>\n",
       "      <td>12623</td>\n",
       "      <td>연교</td>\n",
       "      <td>연남동</td>\n",
       "      <td>4.4</td>\n",
       "      <td>서울특별시 마포구 연남동 383-95  1층</td>\n",
       "      <td>\\r\\nsue\\r\\n</td>\n",
       "      <td>3.8</td>\n",
       "      <td>95</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>어향가지 대박. 소스가 새콤달콤하고 너무 맛있다. 동파육은 쏘쏘.        .....</td>\n",
       "      <td>맛: 좋음</td>\n",
       "      <td>가격: 보통</td>\n",
       "      <td>응대: 보통</td>\n",
       "      <td>동파육, 어향가지</td>\n",
       "      <td>2020년 6월 16일</td>\n",
       "      <td>843</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12624</th>\n",
       "      <td>12624</td>\n",
       "      <td>연교</td>\n",
       "      <td>연남동</td>\n",
       "      <td>4.4</td>\n",
       "      <td>서울특별시 마포구 연남동 383-95  1층</td>\n",
       "      <td>\\r\\n먼지구동이\\r\\n</td>\n",
       "      <td>4.7</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>으이이 다 너무너무 맛있어요\\r\\n또가야지 망두도 면 종류도 다 맛잇어요      ...</td>\n",
       "      <td>맛: 좋음</td>\n",
       "      <td>가격: 만족</td>\n",
       "      <td>응대: 좋음</td>\n",
       "      <td>성쟁바오</td>\n",
       "      <td>2023년 2월 3일</td>\n",
       "      <td>2040</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12625</th>\n",
       "      <td>12625</td>\n",
       "      <td>연교</td>\n",
       "      <td>연남동</td>\n",
       "      <td>4.4</td>\n",
       "      <td>서울특별시 마포구 연남동 383-95  1층</td>\n",
       "      <td>\\r\\nJung Yoon Ha\\r\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>실수로 오픈되어있는 문 들어갔다 민망해하며 나왔습니다. 오픈시간 잘못 안 제 실수도...</td>\n",
       "      <td>맛: 부족</td>\n",
       "      <td>가격: 불만</td>\n",
       "      <td>응대: 나쁨</td>\n",
       "      <td>없음</td>\n",
       "      <td>2022년 7월 30일</td>\n",
       "      <td>187</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12626</th>\n",
       "      <td>12626</td>\n",
       "      <td>연교</td>\n",
       "      <td>연남동</td>\n",
       "      <td>4.4</td>\n",
       "      <td>서울특별시 마포구 연남동 383-95  1층</td>\n",
       "      <td>\\r\\n박만규\\r\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>맛도 없고.. 비싸고.. 불친절함..ㅠㅠ        ...더보기\\r\\n</td>\n",
       "      <td>맛: 부족</td>\n",
       "      <td>가격: 불만</td>\n",
       "      <td>응대: 나쁨</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021년 5월 7일</td>\n",
       "      <td>2219</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11056 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 item_name item_area  item_avg_rating  \\\n",
       "1               1     서령 본점       남대문              4.6   \n",
       "2               2     서령 본점       남대문              4.6   \n",
       "4               4     서령 본점       남대문              4.6   \n",
       "5               5     서령 본점       남대문              4.6   \n",
       "6               6     서령 본점       남대문              4.6   \n",
       "...           ...       ...       ...              ...   \n",
       "12622       12622        연교       연남동              4.4   \n",
       "12623       12623        연교       연남동              4.4   \n",
       "12624       12624        연교       연남동              4.4   \n",
       "12625       12625        연교       연남동              4.4   \n",
       "12626       12626        연교       연남동              4.4   \n",
       "\n",
       "                 item_spec_area             user_name  user_tot_avg_rating  \\\n",
       "1       서울특별시 중구 남대문로5가 120  1층           \\r\\n오또잇\\r\\n                  3.9   \n",
       "2       서울특별시 중구 남대문로5가 120  1층            \\r\\n와퍼\\r\\n                  4.1   \n",
       "4       서울특별시 중구 남대문로5가 120  1층          \\r\\n어먹행먹\\r\\n                  4.2   \n",
       "5       서울특별시 중구 남대문로5가 120  1층  \\r\\n다코미식가\\r\\n먹죽귀\\r\\n                  3.9   \n",
       "6       서울특별시 중구 남대문로5가 120  1층           \\r\\n쿨제이\\r\\n                  3.7   \n",
       "...                         ...                   ...                  ...   \n",
       "12622  서울특별시 마포구 연남동 383-95  1층    \\r\\nmatziplist\\r\\n                  3.4   \n",
       "12623  서울특별시 마포구 연남동 383-95  1층           \\r\\nsue\\r\\n                  3.8   \n",
       "12624  서울특별시 마포구 연남동 383-95  1층         \\r\\n먼지구동이\\r\\n                  4.7   \n",
       "12625  서울특별시 마포구 연남동 383-95  1층  \\r\\nJung Yoon Ha\\r\\n                  1.0   \n",
       "12626  서울특별시 마포구 연남동 383-95  1층           \\r\\n박만규\\r\\n                  1.0   \n",
       "\n",
       "       user_tot_rating_num  user_tot_follow_num  user_rating  \\\n",
       "1                      278                    2          5.0   \n",
       "2                      494                   27          5.0   \n",
       "4                      209                    5          5.0   \n",
       "5                     1339                  669          5.0   \n",
       "6                      118                    4          2.0   \n",
       "...                    ...                  ...          ...   \n",
       "12622                 1187                   23          4.0   \n",
       "12623                   95                    5          4.0   \n",
       "12624                  130                    0          5.0   \n",
       "12625                    1                    0          1.0   \n",
       "12626                    1                    0          1.0   \n",
       "\n",
       "                                              user_query  taste   price  \\\n",
       "1      먹어본 평양냉면 중에 제일 맛있었습니다 다른 곳들은 좀 짜다는 느낌이 강했는데 서령...  맛: 좋음  가격: 보통   \n",
       "2      서울에서 가장 맛있는 평양냉면집.\\r\\n깔끔하고 슴슴한데 이렇게 맛있을 수가 없다....  맛: 좋음  가격: 만족   \n",
       "4      서령 순면(평양냉면) 슴슴한데 은은한 특유의 육수맛이 있어요. 여기는 서령초라는 걸...  맛: 좋음  가격: 만족   \n",
       "5      중구 시청역 남대문 근처에 위치한\\r\\n서령 다녀왔어요예전에 강화도에 있을 때에도 ...  맛: 좋음  가격: 만족   \n",
       "6      평냉계에서 많은 사람들한테 회자되는 '서령'을 갔다왔어요. 결론적으로 제 취향이 아...  맛: 보통  가격: 불만   \n",
       "...                                                  ...    ...     ...   \n",
       "12622  모임하기 좋은 개별적인 공간있어서 좋아보였다. 소룡퍼 맛은 좀 서운했는데 다른것들이...  맛: 좋음  가격: 만족   \n",
       "12623  어향가지 대박. 소스가 새콤달콤하고 너무 맛있다. 동파육은 쏘쏘.        .....  맛: 좋음  가격: 보통   \n",
       "12624  으이이 다 너무너무 맛있어요\\r\\n또가야지 망두도 면 종류도 다 맛잇어요      ...  맛: 좋음  가격: 만족   \n",
       "12625  실수로 오픈되어있는 문 들어갔다 민망해하며 나왔습니다. 오픈시간 잘못 안 제 실수도...  맛: 부족  가격: 불만   \n",
       "12626           맛도 없고.. 비싸고.. 불친절함..ㅠㅠ        ...더보기\\r\\n  맛: 부족  가격: 불만   \n",
       "\n",
       "      service                                        menu          date  \\\n",
       "1      응대: 좋음                     서령 순면, 항정 제육 반 접시(100g)  2025년 7월 31일   \n",
       "2      응대: 좋음                                       서령 순면  2025년 8월 17일   \n",
       "4      응대: 좋음                     서령 순면, 항정 제육 반 접시(100g)   2025년 5월 1일   \n",
       "5      응대: 좋음                                       서령 순면   2025년 2월 1일   \n",
       "6      응대: 보통  서령 순면, 소주, 접시만두 반 접시(3개), 항정 제육 반 접시(100g)  2025년 8월 13일   \n",
       "...       ...                                         ...           ...   \n",
       "12622  응대: 보통                                       샤오롱바오  2020년 3월 30일   \n",
       "12623  응대: 보통                                   동파육, 어향가지  2020년 6월 16일   \n",
       "12624  응대: 좋음                                        성쟁바오   2023년 2월 3일   \n",
       "12625  응대: 나쁨                                          없음  2022년 7월 30일   \n",
       "12626  응대: 나쁨                                         NaN   2021년 5월 7일   \n",
       "\n",
       "       user_id  item_id  \n",
       "1         2826       72  \n",
       "2         2866       72  \n",
       "4         2735       72  \n",
       "5         1470       72  \n",
       "6         3440       72  \n",
       "...        ...      ...  \n",
       "12622      691       91  \n",
       "12623      843       91  \n",
       "12624     2040       91  \n",
       "12625      187       91  \n",
       "12626     2219       91  \n",
       "\n",
       "[11056 rows x 18 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리뷰 개수를 세기 위해 user_id를 기준으로 그룹화\n",
    "user_review_counts = data.groupby('user_id').size()\n",
    "\n",
    "# 리뷰 개수가 2개 이상인 user_id만 필터링\n",
    "users_with_multiple_reviews = user_review_counts[user_review_counts >= 2].index\n",
    "\n",
    "# 해당 user_id를 가진 데이터만 필터링\n",
    "filtered_data = data[data['user_id'].isin(users_with_multiple_reviews)]\n",
    "\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7623eed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\nimport numpy as np\\nfrom collections import Counter\\n# 한국어 형태소 분석기가 필요하다면 konlpy의 Okt나 Mecab을 추천합니다.\\n# from konlpy.tag import Okt\\n# okt = Okt()\\n\\n# 1. 사용자/아이템별 리뷰 통합\\nuser_reviews = data.groupby('user_id')['user_query'].apply(lambda x: ' '.join(x)).reset_index()\\nitem_reviews = data.groupby('item_id')['user_query'].apply(lambda x: ' '.join(x)).reset_index()\\n\\n# 2. 간단한 토큰화 및 단어 사전(Vocabulary) 생성 함수\\ndef build_vocab(text_series, max_vocab_size=10000):\\n    words = []\\n    for text in text_series:\\n        # 실제 환경에서는 형태소 분석기(okt.morphs)를 사용하는 것이 성능에 훨씬 좋습니다.\\n        words.extend(text.split()) \\n    \\n    counter = Counter(words)\\n    vocab = {word: i+2 for i, (word, _) in enumerate(counter.most_common(max_vocab_size))}\\n    vocab['<PAD>'] = 0 # 패딩용\\n    vocab['<UNK>'] = 1 # 모르는 단어용\\n    return vocab\\n\\n# 3. 텍스트를 정수 인덱스로 변환하고 패딩하는 함수\\ndef text_to_sequence(text_series, vocab, max_length=500):\\n    sequences = []\\n    for text in text_series:\\n        seq = [vocab.get(word, vocab['<UNK>']) for word in text.split()]\\n        # Truncating & Padding\\n        if len(seq) > max_length:\\n            seq = seq[:max_length]\\n        else:\\n            seq = seq + [vocab['<PAD>']] * (max_length - len(seq))\\n        sequences.append(seq)\\n    return np.array(sequences)\\n\\n# 단어 사전 구축 및 시퀀스 변환 (사용자, 아이템 전체 리뷰를 합쳐서 하나의 사전을 만드는 것이 일반적입니다)\\nall_texts = pd.concat([user_reviews['user_query'], item_reviews['user_query']])\\nvocab = build_vocab(all_texts)\\n\\nuser_sequences = text_to_sequence(user_reviews['user_query'], vocab)\\nitem_sequences = text_to_sequence(item_reviews['user_query'], vocab)\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "# 한국어 형태소 분석기가 필요하다면 konlpy의 Okt나 Mecab을 추천합니다.\n",
    "# from konlpy.tag import Okt\n",
    "# okt = Okt()\n",
    "\n",
    "# 1. 사용자/아이템별 리뷰 통합\n",
    "user_reviews = data.groupby('user_id')['user_query'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "item_reviews = data.groupby('item_id')['user_query'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# 2. 간단한 토큰화 및 단어 사전(Vocabulary) 생성 함수\n",
    "def build_vocab(text_series, max_vocab_size=10000):\n",
    "    words = []\n",
    "    for text in text_series:\n",
    "        # 실제 환경에서는 형태소 분석기(okt.morphs)를 사용하는 것이 성능에 훨씬 좋습니다.\n",
    "        words.extend(text.split()) \n",
    "    \n",
    "    counter = Counter(words)\n",
    "    vocab = {word: i+2 for i, (word, _) in enumerate(counter.most_common(max_vocab_size))}\n",
    "    vocab['<PAD>'] = 0 # 패딩용\n",
    "    vocab['<UNK>'] = 1 # 모르는 단어용\n",
    "    return vocab\n",
    "\n",
    "# 3. 텍스트를 정수 인덱스로 변환하고 패딩하는 함수\n",
    "def text_to_sequence(text_series, vocab, max_length=500):\n",
    "    sequences = []\n",
    "    for text in text_series:\n",
    "        seq = [vocab.get(word, vocab['<UNK>']) for word in text.split()]\n",
    "        # Truncating & Padding\n",
    "        if len(seq) > max_length:\n",
    "            seq = seq[:max_length]\n",
    "        else:\n",
    "            seq = seq + [vocab['<PAD>']] * (max_length - len(seq))\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# 단어 사전 구축 및 시퀀스 변환 (사용자, 아이템 전체 리뷰를 합쳐서 하나의 사전을 만드는 것이 일반적입니다)\n",
    "all_texts = pd.concat([user_reviews['user_query'], item_reviews['user_query']])\n",
    "vocab = build_vocab(all_texts)\n",
    "\n",
    "user_sequences = text_to_sequence(user_reviews['user_query'], vocab)\n",
    "item_sequences = text_to_sequence(item_reviews['user_query'], vocab)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcc33731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유저 및 식당별 리뷰 통합 중...\n",
      "형태소 분석 및 Tokenization processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "user docs tokenization: 100%|██████████| 3781/3781 [00:46<00:00, 80.53it/s] \n",
      "item docs tokenization: 100%|██████████| 160/160 [00:34<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장 생성 중...\n",
      "완성된 단어장 크기: 11446\n",
      "정수 시퀀스 변환 및 패딩 적용 중...\n",
      "전처리 완료! user_seq_dict와 item_seq_dict가 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "jvm_path = r'C:\\Program Files\\Java\\jdk-25.0.2\\bin\\server\\jvm.dll'\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-25.0.2'   # 꼭 추가\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# --- 하이퍼파라미터 설정 ---\n",
    "MAX_VOCAB_SIZE = 30000  # 단어장 최대 크기 (빈도수 상위 5만개)\n",
    "MAX_DOC_LENGTH = 550   # 하나의 문서(리뷰 모음)에 들어갈 최대 단어 수\n",
    "\n",
    "# 1. 유저별, 아이템(식당)별로 리뷰 텍스트 하나로 합치기\n",
    "print(\"유저 및 식당별 리뷰 통합 중...\")\n",
    "user_docs = data.groupby('user_id')['user_query'].apply(lambda x: ' '.join(map(str, x))).reset_index()\n",
    "item_docs = data.groupby('item_id')['user_query'].apply(lambda x: ' '.join(map(str, x))).reset_index()\n",
    "\n",
    "print('형태소 분석 및 Tokenization processing')\n",
    "tqdm.pandas(desc='user docs tokenization')\n",
    "user_docs['tokenized'] = user_docs['user_query'].progress_apply(lambda x: okt.morphs(x,stem=True))\n",
    "tqdm.pandas(desc='item docs tokenization')\n",
    "item_docs['tokenized'] = item_docs['user_query'].progress_apply(lambda x: okt.morphs(x,stem=True))\n",
    "\n",
    "\n",
    "# 2. 토큰화 및 단어장(Vocabulary) 생성\n",
    "print(\"단어장 생성 중...\")\n",
    "all_tokens =[]\n",
    "for tokens in user_docs['tokenized']:\n",
    "    all_tokens.extend(tokens)\n",
    "for tokens in item_docs['tokenized']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# 빈도수 순으로 단어 정렬 및 인덱스 부여 (0은 PAD, 1은 UNK)\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "for i, (word, count) in enumerate(word_counts.most_common(MAX_VOCAB_SIZE)):\n",
    "    vocab[word] = i + 2  # 0과 1은 이미 차지했으므로 2부터 시작\n",
    "\n",
    "print(f\"완성된 단어장 크기: {len(vocab)}\")\n",
    "\n",
    "# 3. 텍스트를 정수 시퀀스로 변환하고 패딩(Padding)하는 함수\n",
    "def text_to_padded_sequence(text, vocab, max_length):\n",
    "    # 단어가 사전에 없으면 <UNK>(1)로 처리\n",
    "    seq = [vocab.get(word, vocab['<UNK>']) for word in text.split()]\n",
    "    \n",
    "    # 길이에 맞게 자르거나(Truncate) 패딩(Padding)\n",
    "    if len(seq) > max_length:\n",
    "        return seq[:max_length] # 최대 길이보다 길면 자르기\n",
    "    else:\n",
    "        return seq + [vocab['<PAD>']] * (max_length - len(seq)) # 짧으면 0으로 채우기\n",
    "\n",
    "# 4. 유저와 아이템의 문서를 정수 시퀀스로 변환하여 딕셔너리로 저장\n",
    "print(\"정수 시퀀스 변환 및 패딩 적용 중...\")\n",
    "user_seq_dict = {}\n",
    "for _, row in user_docs.iterrows():\n",
    "    user_seq_dict[row['user_id']] = text_to_padded_sequence(row['user_query'], vocab, MAX_DOC_LENGTH)\n",
    "\n",
    "item_seq_dict = {}\n",
    "for _, row in item_docs.iterrows():\n",
    "    item_seq_dict[row['item_id']] = text_to_padded_sequence(row['user_query'], vocab, MAX_DOC_LENGTH)\n",
    "\n",
    "print(\"전처리 완료! user_seq_dict와 item_seq_dict가 생성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6294aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# pip install gensim\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "# 1. 사전 학습된 한국어 임베딩 모델 로드 (Korean fastText-from Meta)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(\"C:/Users/hanan/Downloads/cc.ko.300.vec.gz\",binary=False,limit=30000)\n",
    "\n",
    "EMBED_DIM = 300 \n",
    "vocab_size = len(vocab) # 이전 단계에서 만든 단어 사전의 크기\n",
    "\n",
    "# 2. PyTorch Embedding 레이어에 넣을 빈 가중치 행렬 생성\n",
    "# 형태: (단어 사전 크기, 임베딩 차원)\n",
    "embedding_matrix = np.zeros((vocab_size, EMBED_DIM))\n",
    "\n",
    "# 3. 단어 사전(vocab)을 순회하며 가중치 행렬 채우기\n",
    "for word, idx in vocab.items():\n",
    "    if word in word2vec_model:\n",
    "        # 사전 학습된 모델에 단어가 존재하면 해당 벡터를 가져옴\n",
    "        embedding_matrix[idx] = word2vec_model[word]\n",
    "    else:\n",
    "        # 모델에 없는 단어(OOV)는 정규분포를 따르는 랜덤 벡터로 초기화하거나 0으로 둡니다.\n",
    "        if word == '<PAD>':\n",
    "            embedding_matrix[idx] = np.zeros(EMBED_DIM)\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(EMBED_DIM,))\n",
    "\n",
    "# Numpy 배열을 PyTorch Tensor로 변환\n",
    "embedding_tensor = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e18c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DiningCodeDataset(Dataset):\n",
    "    def __init__(self, df, user_seq_dict, item_seq_dict):\n",
    "        self.users = df['user_id'].values\n",
    "        self.items = df['item_id'].values\n",
    "        self.ratings = df['user_rating'].values  # 예측해야 할 정답 평점\n",
    "        \n",
    "        self.user_seq_dict = user_seq_dict\n",
    "        self.item_seq_dict = item_seq_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.users[idx]\n",
    "        item_id = self.items[idx]\n",
    "        rating = self.ratings[idx]\n",
    "\n",
    "        # 딕셔너리에서 미리 만들어둔 정수 시퀀스를 가져옴\n",
    "        u_doc = self.user_seq_dict[user_id]\n",
    "        i_doc = self.item_seq_dict[item_id]\n",
    "\n",
    "        # PyTorch 모델 입력용 텐서로 변환 (타입 변환이 매우 중요합니다)\n",
    "        u_doc_tensor = torch.tensor(u_doc, dtype=torch.long)\n",
    "        i_doc_tensor = torch.tensor(i_doc, dtype=torch.long)\n",
    "        rating_tensor = torch.tensor(rating, dtype=torch.float32)\n",
    "\n",
    "        return u_doc_tensor, i_doc_tensor, rating_tensor\n",
    "\n",
    "# 데이터로더 생성 (한 번에 64개씩 묶어서 모델에 전달)\n",
    "BATCH_SIZE = 64\n",
    "dataset = DiningCodeDataset(data, user_seq_dict, item_seq_dict)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepCoNN(nn.Module):\n",
    "    def __init__(self,config,embedding_matrix):\n",
    "        super(DeepCoNN,self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        #1.Embedding layer\n",
    "        vocab_size,embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(\n",
    "            torch.FloatTensor(embedding_matrix)\n",
    "        )\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        #2-1.User Network\n",
    "        self.user_cnn = nn.Conv1d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=config.num_filters,\n",
    "            kernel_size=config.kernel_size,\n",
    "            padding=1\n",
    "        )\n",
    "        self.user_fc = nn.Linear(config.num_filters,config.latent_dim)\n",
    "\n",
    "        #2-2. Item Network\n",
    "        self.item_cnn = nn.Conv1d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=config.num_filters,\n",
    "            kernel_size=config.kernel_size,\n",
    "            padding=1\n",
    "        )\n",
    "        self.item_fc = nn.Linear(config.num_filters,config.latent_dim)\n",
    "\n",
    "        #Dropout\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "        #3.FM Layer (2nd-order latent vectors)\n",
    "        self.fm_linear = nn.Linear(config.latent_dim*2,1)\n",
    "        \n",
    "        self.fm_V = nn.Parameter(\n",
    "            torch.rand(config.latent_dim*2,config.fm_k)\n",
    "        )\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self,user_doc,item_doc):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_doc: (batch_size,max_doc_length)\n",
    "            item_doc: (batch_size,max_doc_length)\n",
    "\n",
    "        Returns:\n",
    "            rating: (batch_size,)\n",
    "        \"\"\"\n",
    "\n",
    "        #User_Net\n",
    "        user_emb = self.embedding(user_doc) # (B,L,E)\n",
    "        user_emb = user_emb.transpose(1,2) # (B,E,L) - conv1d input format\n",
    "\n",
    "        user_conv = F.relu(self.user_cnn(user_emb)) \n",
    "        user_pool = F.max_pool1d(user_conv,kernel_size=user_conv.size(2))\n",
    "        user_pool = user_pool.squeeze(2)\n",
    "\n",
    "        user_latent = F.relu(self.user_fc(user_pool))\n",
    "        user_latent = self.dropout(user_latent)\n",
    "\n",
    "        #item_Net\n",
    "        item_emb = self.embedding(item_doc) # (B,L,E)\n",
    "        item_emb = item_emb.transpose(1,2) # (B,E,L) - conv1d input format\n",
    "\n",
    "        item_conv = F.relu(self.item_cnn(item_emb)) \n",
    "        item_pool = F.max_pool1d(item_conv,kernel_size=item_conv.size(2))\n",
    "        item_pool = item_pool.squeeze(2)\n",
    "\n",
    "        item_latent = F.relu(self.item_fc(item_pool))\n",
    "        item_latent = self.dropout(item_latent)\n",
    "\n",
    "        #concatenate\n",
    "        z = torch.cat([user_latent,item_latent],dim=1)\n",
    "\n",
    "        #FM layer\n",
    "        linear_term = self.fm_linear(z)\n",
    "\n",
    "        interactions = torch.mm(z,self.fm_V)\n",
    "        interactions_squared = torch.mm(z**2,self.fm_V**2)\n",
    "\n",
    "        quadratic_term = 0.5*torch.sum(\n",
    "            interactions**2-interactions_squared,dim=1,keepdim=True\n",
    "        )\n",
    "\n",
    "        #Predict\n",
    "        rating = self.global_bias + linear_term.squeeze(1) + quadratic_term.squeeze(1)\n",
    "        \n",
    "        return rating\n",
    "\n",
    "class Config:\n",
    "\n",
    "    max_doc_length = 550\n",
    "    embedding_dim = 300\n",
    "\n",
    "    \"Model Parameters\"\n",
    "    num_filters = 100\n",
    "    kernel_size = 3\n",
    "    latent_dim = 50\n",
    "    fm_k = 8\n",
    "\n",
    "    \"Traning\"\n",
    "\n",
    "    batch_size = 64\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.002\n",
    "    dropout_rate = 0.5\n",
    "\n",
    "    \"Other\"\n",
    "    device='cuda'\n",
    "    random_seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba57b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "config = Config()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 중인 디바이스: {device}\")\n",
    "model = DeepCoNN(config=config, embedding_matrix=embedding_matrix).to(device)\n",
    "\n",
    "# 평점 예측(회귀) 문제이므로 손실 함수는 MSE(Mean Squared Error) 사용\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 모델의 가중치를 업데이트할 최적화 함수 (Adam이 가장 무난합니다)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35859088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/198], Loss: 4.3914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 2. 순전파 (Forward Pass): 예측 평점 계산\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 이 부분이 작성하신 DeepCoNN 클래스의 forward 함수를 실행하는 부분입니다.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_user\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_item\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# 3. 오차 계산 (실제 평점과 예측 평점의 차이)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, batch_rating)\n",
      "File \u001b[1;32mc:\\Users\\hanan\\miniconda3\\envs\\NLP_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hanan\\miniconda3\\envs\\NLP_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[18], line 61\u001b[0m, in \u001b[0;36mDeepCoNN.forward\u001b[1;34m(self, user_doc, item_doc)\u001b[0m\n\u001b[0;32m     58\u001b[0m user_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(user_doc) \u001b[38;5;66;03m# (B,L,E)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m user_emb \u001b[38;5;241m=\u001b[39m user_emb\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B,E,L) - conv1d input format\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m user_conv \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_emb\u001b[49m\u001b[43m)\u001b[49m) \n\u001b[0;32m     62\u001b[0m user_pool \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool1d(user_conv,kernel_size\u001b[38;5;241m=\u001b[39muser_conv\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     63\u001b[0m user_pool \u001b[38;5;241m=\u001b[39m user_pool\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hanan\\miniconda3\\envs\\NLP_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hanan\\miniconda3\\envs\\NLP_test\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\hanan\\miniconda3\\envs\\NLP_test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hanan\\miniconda3\\envs\\NLP_test\\lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # 모델을 학습 모드로 설정 (Dropout 등이 활성화됨)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (batch_user, batch_item, batch_rating) in enumerate(dataloader):\n",
    "        # 데이터를 GPU(또는 CPU)로 이동\n",
    "        batch_user = batch_user.to(device)\n",
    "        batch_item = batch_item.to(device)\n",
    "        batch_rating = batch_rating.to(device)\n",
    "        \n",
    "        # 1. 기울기 초기화 (이전 배치의 기울기가 누적되지 않도록)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. 순전파 (Forward Pass): 예측 평점 계산\n",
    "        # 이 부분이 작성하신 DeepCoNN 클래스의 forward 함수를 실행하는 부분입니다.\n",
    "        predictions = model(batch_user, batch_item)\n",
    "        \n",
    "        # 3. 오차 계산 (실제 평점과 예측 평점의 차이)\n",
    "        loss = criterion(predictions, batch_rating)\n",
    "        \n",
    "        # 4. 역전파 (Backward Pass): 기울기 계산\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. 가중치 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"==== Epoch {epoch+1} 완료 | 평균 Loss: {avg_loss:.4f} ====\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9981f3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from gensim import models\\n\\nmodel = KeyedVectors.load_word2vec_format(\"C:/Users/hanan/Downloads/cc.ko.300.vec.gz\",binary=False,limit=20000)\\nfor w, sim in model.similar_by_word(\\'존맛\\', 10):\\n    print(f\\'{w}: {sim}\\')\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from gensim import models\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"C:/Users/hanan/Downloads/cc.ko.300.vec.gz\",binary=False,limit=20000)\n",
    "for w, sim in model.similar_by_word('존맛', 10):\n",
    "    print(f'{w}: {sim}')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
