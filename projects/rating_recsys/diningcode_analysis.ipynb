{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe74f02",
   "metadata": {},
   "source": [
    "# 다이닝 코드 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af865bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('diningcode_data_crawling_20260125_1542.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311017ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12627, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(columns=['Unnamed: 0'],axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15acca40",
   "metadata": {},
   "source": [
    "## Preprocessing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24e6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다코 미식가 / user_name 분리\n",
    "daco_list = dict()\n",
    "name_list = list()\n",
    "for i,val in enumerate(data['user_name']):\n",
    "    if \"다코미식가\" in val:\n",
    "        daco_list[i] = 1\n",
    "    else:\n",
    "        daco_list[i] = 0\n",
    "    name_list.append(data['user_name'][i].replace('다코미식가',''))\n",
    "data['daco_gourmand'] = daco_list\n",
    "\n",
    "name_list = [name.replace('\\r\\n', '') for name in name_list]\n",
    "name_list = [name.replace('\\n', '') for name in name_list]\n",
    "data['user_name'] = name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0a4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user_rating'] = data['user_rating'].str.replace('점','').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12efec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals = data['taste'].unique()\n",
    "mapping_dict = {\n",
    "    unique_vals[0]:3,\n",
    "    unique_vals[1]:2,\n",
    "    unique_vals[2]:1\n",
    "}\n",
    "data['taste_enc'] = data['taste'].map(mapping_dict)\n",
    "del mapping_dict, unique_vals\n",
    "\n",
    "unique_vals = data['service'].unique()\n",
    "mapping_dict = {\n",
    "    unique_vals[0]:3,\n",
    "    unique_vals[1]:2,\n",
    "    unique_vals[2]:1\n",
    "}\n",
    "data['service_enc'] = data['service'].map(mapping_dict)\n",
    "del mapping_dict, unique_vals\n",
    "\n",
    "unique_vals = data['price'].unique()\n",
    "mapping_dict = {\n",
    "    unique_vals[0]:3,\n",
    "    unique_vals[1]:2,\n",
    "    unique_vals[2]:1\n",
    "}\n",
    "data['price_enc'] = data['price'].map(mapping_dict)\n",
    "del mapping_dict, unique_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e744005",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_le = LabelEncoder()\n",
    "item_le = LabelEncoder()\n",
    "\n",
    "data['user_id'] = user_le.fit_transform(data['user_name'])\n",
    "data['item_id'] = item_le.fit_transform(data['item_name'])\n",
    "\n",
    "# User name-id mapping\n",
    "user_i2n = dict(enumerate(user_le.classes_))  \n",
    "user_n2i = {v: k for k, v in user_i2n.items()}\n",
    "\n",
    "item_i2n = dict(enumerate(item_le.classes_))\n",
    "item_n2i = {v: k for k, v in item_i2n.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a4709c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2208)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(data['user_name'].value_counts() > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3601c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "593fbbc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8723, 21)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc013f1",
   "metadata": {},
   "source": [
    "## Project_Main (Founding Recommnedation model)\n",
    "1. Baseline (DeepFM or MF model)\n",
    "2. DeepCoNN + MF\n",
    "3. LLM (KoBERT,Gemma) + MF model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981f2a7",
   "metadata": {},
   "source": [
    "### 1. Baseline (Matrix Factorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "268b3fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity :  0.9855843038644786\n",
      "user_item_mat shape : (160, 3778)\n"
     ]
    }
   ],
   "source": [
    "user_item_mat = pd.pivot_table(data=data,index='item_id',\n",
    "                               columns='user_id',values='user_rating')\n",
    "print('sparsity : ',np.count_nonzero(np.isnan(user_item_mat)) / user_item_mat.size)\n",
    "print(f'user_item_mat shape : {user_item_mat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dfc85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_data = data[['user_name','item_name','user_rating']]\n",
    "mf_data.columns = ['user','item','rating']\n",
    "\n",
    "mf_train = mf_data.groupby('user').sample(frac=0.8,random_state=42)\n",
    "mf_train_ind = mf_train.index\n",
    "mf_test = mf_data.drop(mf_train_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e4e273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 , Loss : 2222.281957 , Rooted Loss: 47.14\n",
      "Epoch : 10 , Loss : 1895.829478 , Rooted Loss: 43.54\n",
      "Epoch : 20 , Loss : 1780.056921 , Rooted Loss: 42.19\n",
      "Epoch : 30 , Loss : 1723.511763 , Rooted Loss: 41.52\n",
      "Epoch : 40 , Loss : 1691.422434 , Rooted Loss: 41.13\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')  # Go up two levels to reach the directory containing 'Study'\n",
    "\n",
    "from Study.RecSys.matrixfactorization import matfac\n",
    "\n",
    "k=10\n",
    "lr=0.001\n",
    "reg_param = 0.02\n",
    "epochs=50\n",
    "\n",
    "mf_model = matfac.MatrixFactorization(k,lr,reg_param,epochs)\n",
    "\n",
    "P,Q,b_u,b_i = mf_model.fit(mf_train)\n",
    "mf_pred,mf_test = mf_model.predict(mf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3645eeae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9926146584105436)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mf_pred\n",
    "y_true = mf_test\n",
    "k=10\n",
    "ndcg_k = []\n",
    "for user_num in y_pred['user'].unique():\n",
    "    top_pred_items = y_pred.loc[(y_pred['user']==user_num)].sort_values('rating',ascending=False)\n",
    "    pred_sequence = top_pred_items['item'][:k].values\n",
    "\n",
    "    test_items = y_true.loc[y_true['user']==user_num]\n",
    "    ideal_rel_score = test_items.sort_values('rating',ascending=False)[:k]['rating'].values\n",
    "    rel_score = test_items.set_index('item').reindex(pred_sequence)['rating'].values\n",
    "    dcg_k = np.sum((np.pow(2,rel_score) -1) / np.log2(np.arange(2,len(rel_score)+2)))\n",
    "    idcg_k = np.sum((np.pow(2,ideal_rel_score) -1) / np.log2(np.arange(2,len(ideal_rel_score)+2)))\n",
    "    ndcg_k.append(dcg_k / idcg_k if idcg_k>0 else 0)\n",
    "np.mean(ndcg_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11807fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6668,Precision@K: 0.1250,Recall@K: 1.0000,NDCG@K: 0.9926\n"
     ]
    }
   ],
   "source": [
    "from Study.RecSys.matrixfactorization.preprocessing import precision_at_k,ndcg_at_k,recall_at_k\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "rmse_par = root_mean_squared_error(mf_pred['rating'].values,mf_test['rating'].values)\n",
    "prec_at_k_par = precision_at_k(mf_pred,mf_test,k=10)\n",
    "rec_at_k_par = recall_at_k(mf_pred,mf_test,k=10)\n",
    "ndcg_at_k_par = ndcg_at_k(mf_pred,mf_test,k=10)\n",
    "\n",
    "print(f'RMSE: {rmse_par:.4f},Precision@K: {prec_at_k_par:.4f},Recall@K: {rec_at_k_par:.4f},NDCG@K: {ndcg_at_k_par:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79fbc96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User '먹죽귀' Top-10 Recommendation ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>predicted_rating</th>\n",
       "      <th>actual_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6709</th>\n",
       "      <td>베트남시장쌀국수</td>\n",
       "      <td>4.925535</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6765</th>\n",
       "      <td>벱</td>\n",
       "      <td>4.756190</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9232</th>\n",
       "      <td>타오 마라탕</td>\n",
       "      <td>4.724055</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>오레노라멘 본점</td>\n",
       "      <td>4.503047</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>서령 본점</td>\n",
       "      <td>4.438923</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>런던베이글뮤지엄 잠실점</td>\n",
       "      <td>4.312912</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>오레노라멘 롯데월드몰점</td>\n",
       "      <td>4.197145</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           item_id  predicted_rating  actual_rating\n",
       "6709      베트남시장쌀국수          4.925535            4.0\n",
       "6765             벱          4.756190            5.0\n",
       "9232        타오 마라탕          4.724055            4.0\n",
       "1359      오레노라멘 본점          4.503047            5.0\n",
       "5            서령 본점          4.438923            5.0\n",
       "4873  런던베이글뮤지엄 잠실점          4.312912            3.5\n",
       "1721  오레노라멘 롯데월드몰점          4.197145            4.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_p_recommendations(df, user_id, k=5):\n",
    "    user_data = df[df['user_id'] == user_id]\n",
    "\n",
    "    top_k_items = user_data.sort_values(by='predicted_rating', ascending=False).head(k)\n",
    "    \n",
    "    print(f\"=== User '{user_id}' Top-{k} Recommendation ===\")\n",
    "    return top_k_items[['item_id', 'predicted_rating', 'actual_rating']]\n",
    "\n",
    "topn_df = get_top_p_recommendations(comparison_df, '먹죽귀', k=10)\n",
    "topn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d94a326c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_rating</th>\n",
       "      <th>actual_rating</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.438923</td>\n",
       "      <td>5.0</td>\n",
       "      <td>먹죽귀</td>\n",
       "      <td>서령 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.643927</td>\n",
       "      <td>2.0</td>\n",
       "      <td>쿨제이</td>\n",
       "      <td>서령 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.768596</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Ssuworld</td>\n",
       "      <td>서령 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.557831</td>\n",
       "      <td>5.0</td>\n",
       "      <td>쵸옹</td>\n",
       "      <td>서령 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.719772</td>\n",
       "      <td>4.5</td>\n",
       "      <td>오렌지9191</td>\n",
       "      <td>서령 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.623751</td>\n",
       "      <td>5.0</td>\n",
       "      <td>meraki</td>\n",
       "      <td>서령 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.824371</td>\n",
       "      <td>5.0</td>\n",
       "      <td>아제나의맛집탐방</td>\n",
       "      <td>서령 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4.871349</td>\n",
       "      <td>4.0</td>\n",
       "      <td>브라우니쿠키</td>\n",
       "      <td>서령 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.581752</td>\n",
       "      <td>5.0</td>\n",
       "      <td>이거맛있당</td>\n",
       "      <td>서령 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>4.435929</td>\n",
       "      <td>5.0</td>\n",
       "      <td>데이지</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4.238395</td>\n",
       "      <td>4.0</td>\n",
       "      <td>평냉선비</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4.332549</td>\n",
       "      <td>5.0</td>\n",
       "      <td>크뀨</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4.331051</td>\n",
       "      <td>5.0</td>\n",
       "      <td>조이2</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>4.286672</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Terra-j</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>4.097944</td>\n",
       "      <td>4.0</td>\n",
       "      <td>욤뇸뇸</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>4.200445</td>\n",
       "      <td>5.0</td>\n",
       "      <td>masitsum</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>4.261160</td>\n",
       "      <td>4.0</td>\n",
       "      <td>빵꾸다이</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>4.304595</td>\n",
       "      <td>5.0</td>\n",
       "      <td>문덕이</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>4.225337</td>\n",
       "      <td>4.0</td>\n",
       "      <td>희님</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>4.241084</td>\n",
       "      <td>4.0</td>\n",
       "      <td>맛주먹</td>\n",
       "      <td>우래옥 본점</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     predicted_rating  actual_rating   user_id item_id\n",
       "5            4.438923            5.0       먹죽귀   서령 본점\n",
       "6            4.643927            2.0       쿨제이   서령 본점\n",
       "12           4.768596            5.0  Ssuworld   서령 본점\n",
       "18           4.557831            5.0        쵸옹   서령 본점\n",
       "22           4.719772            4.5   오렌지9191   서령 본점\n",
       "36           4.623751            5.0    meraki   서령 본점\n",
       "39           4.824371            5.0  아제나의맛집탐방   서령 본점\n",
       "46           4.871349            4.0    브라우니쿠키   서령 본점\n",
       "47           4.581752            5.0     이거맛있당   서령 본점\n",
       "50           4.435929            5.0       데이지  우래옥 본점\n",
       "55           4.238395            4.0      평냉선비  우래옥 본점\n",
       "81           4.332549            5.0        크뀨  우래옥 본점\n",
       "103          4.331051            5.0       조이2  우래옥 본점\n",
       "107          4.286672            4.0   Terra-j  우래옥 본점\n",
       "120          4.097944            4.0       욤뇸뇸  우래옥 본점\n",
       "124          4.200445            5.0  masitsum  우래옥 본점\n",
       "131          4.261160            4.0      빵꾸다이  우래옥 본점\n",
       "132          4.304595            5.0       문덕이  우래옥 본점\n",
       "137          4.225337            4.0        희님  우래옥 본점\n",
       "147          4.241084            4.0       맛주먹  우래옥 본점"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'predicted_rating': mf_pred['rating'].values,\n",
    "    'actual_rating': mf_test['rating'].values,\n",
    "    'user_id' : mf_test['user'],\n",
    "    'item_id' : mf_test['item']\n",
    "})\n",
    "comparison_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e396228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef ndcg_at_k(y_true, y_score, k=-1): #Vector의 계산으로 이루어짐\\n    y_true = np.array(y_true)\\n    y_score = np.array(y_score)\\n    n = len(y_true)\\n    # k가 -1 또는 데이터 전체보다 크면 k=n으로 보정\\n    if k == -1 or k > n:\\n        k = n\\n\\n    order = np.argsort(y_score)[::-1]\\n    y_true_sorted = y_true[order[:k]]  # k 길이만큼만 자름\\n    dcg = np.sum((2 ** y_true_sorted - 1) / np.log2(np.arange(2, k + 2)))\\n    best_dcg = np.sum((2 ** np.sort(y_true)[::-1][:k] - 1) / np.log2(np.arange(2, k + 2)))\\n\\n    return dcg / best_dcg if best_dcg > 0 else 0.0\\n\\nndcg_k = []\\nfor i,col in enumerate(R_pred):\\n    ndcg_k.append(ndcg_at_k(R[i],R_pred[i],k=-1))\\nprint('ndcg@k mean:' , np.mean(ndcg_k))\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def ndcg_at_k(y_true, y_score, k=-1): #Vector의 계산으로 이루어짐\n",
    "    y_true = np.array(y_true)\n",
    "    y_score = np.array(y_score)\n",
    "    n = len(y_true)\n",
    "    # k가 -1 또는 데이터 전체보다 크면 k=n으로 보정\n",
    "    if k == -1 or k > n:\n",
    "        k = n\n",
    "\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true_sorted = y_true[order[:k]]  # k 길이만큼만 자름\n",
    "    dcg = np.sum((2 ** y_true_sorted - 1) / np.log2(np.arange(2, k + 2)))\n",
    "    best_dcg = np.sum((2 ** np.sort(y_true)[::-1][:k] - 1) / np.log2(np.arange(2, k + 2)))\n",
    "\n",
    "    return dcg / best_dcg if best_dcg > 0 else 0.0\n",
    "\n",
    "ndcg_k = []\n",
    "for i,col in enumerate(R_pred):\n",
    "    ndcg_k.append(ndcg_at_k(R[i],R_pred[i],k=-1))\n",
    "print('ndcg@k mean:' , np.mean(ndcg_k))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df249bda",
   "metadata": {},
   "source": [
    "### 2. DeepCoNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1de39c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_data = data[['user_id','item_id','user_rating','user_query']]\n",
    "dc_data.columns = ['user_id','item_id','rating','review']\n",
    "\n",
    "dc_train = dc_data.groupby('user_id').sample(frac=0.8,random_state=42)\n",
    "dc_train_ind = dc_train.index\n",
    "dc_test = dc_data.drop(dc_train_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "488a1efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "809b9b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morpheme(형태소) Analysis & Tokenization processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "user docs tokenization: 100%|██████████| 3778/3778 [00:26<00:00, 143.16it/s]\n",
      "item docs tokenization: 100%|██████████| 160/160 [00:18<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Dict...\n",
      "Dict size: 10791\n",
      "\n",
      "Morpheme(형태소) Analysis & Tokenization processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "user docs tokenization: 100%|██████████| 964/964 [00:03<00:00, 248.32it/s]\n",
      "item docs tokenization: 100%|██████████| 155/155 [00:03<00:00, 42.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Dict...\n",
      "Dict size: 4518\n",
      "\n",
      "Apply padding and int sequence\n",
      "Apply padding and int sequence\n",
      "Preprocessing Complete\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "jvm_path = r'C:\\Program Files\\Java\\jdk-23\\bin\\server\\jvm.dll' # r'C:\\Program Files\\Java\\jdk-25.0.2\\bin\\server\\jvm.dll'\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-23'   \n",
    "\n",
    "# --- 하이퍼파라미터 설정 ---\n",
    "MAX_VOCAB_SIZE = 30000  # Top N\n",
    "MAX_DOC_LENGTH = 550   \n",
    "\n",
    "def tokeniz(data):\n",
    "    okt = Okt()\n",
    "    \n",
    "    # 1. Construct User-Item document\n",
    "    user_docs = data.groupby('user_id')['review'].apply(lambda x: ' '.join(map(str, x))).reset_index()\n",
    "    item_docs = data.groupby('item_id')['review'].apply(lambda x: ' '.join(map(str, x))).reset_index()\n",
    "\n",
    "    print('Morpheme(형태소) Analysis & Tokenization processing')\n",
    "\n",
    "    tqdm.pandas(desc='user docs tokenization')\n",
    "    user_docs['tokenized'] = user_docs['review'].progress_apply(lambda x: okt.morphs(x,stem=True))\n",
    "\n",
    "    tqdm.pandas(desc='item docs tokenization')\n",
    "    item_docs['tokenized'] = item_docs['review'].progress_apply(lambda x: okt.morphs(x,stem=True))\n",
    "\n",
    "    # 2. Tokenization / generate Vocab dictionary\n",
    "    print(\"Generating Dict...\")\n",
    "    all_tokens =[]\n",
    "    for tokens in user_docs['tokenized']:\n",
    "        all_tokens.extend(tokens)\n",
    "    for tokens in item_docs['tokenized']:\n",
    "        all_tokens.extend(tokens)\n",
    "    word_counts = Counter(all_tokens)\n",
    "\n",
    "    # 0-<PAD> / 1-<UNK>\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for i, (word, count) in enumerate(word_counts.most_common(MAX_VOCAB_SIZE)):\n",
    "        vocab[word] = i + 2  # 0과 1은 이미 차지했으므로 2부터 시작\n",
    "\n",
    "    print(f\"Dict size: {len(vocab)}\\n\")\n",
    "\n",
    "    return vocab,user_docs,item_docs\n",
    "\n",
    "# 3. Padding\n",
    "def text_to_padded_sequence(text, vocab, max_length):\n",
    "    # 단어가 사전에 없으면 <UNK>(1)로 처리\n",
    "    seq = [vocab.get(word, vocab['<UNK>']) for word in text.split()]\n",
    "    \n",
    "    # Length Adjustment\n",
    "    if len(seq) > max_length:\n",
    "        return seq[:max_length] # Truncation\n",
    "    else:\n",
    "        return seq + [vocab['<PAD>']] * (max_length - len(seq)) # Padding\n",
    "\n",
    "tr_vocab,dc_tr_user_doc,dc_tr_item_doc = tokeniz(dc_train)\n",
    "te_vocab,dc_te_user_doc,dc_te_item_doc = tokeniz(dc_test)\n",
    "\n",
    "# 4. UI sequence\n",
    "def app_padding(vocab):\n",
    "    print(\"Apply padding and int sequence\")\n",
    "    user_seq_dict = {}\n",
    "    for _, row in dc_tr_user_doc.iterrows():\n",
    "        user_seq_dict[row['user_id']] = text_to_padded_sequence(row['review'], vocab, MAX_DOC_LENGTH)\n",
    "\n",
    "    item_seq_dict = {}\n",
    "    for _, row in dc_tr_item_doc.iterrows():\n",
    "        item_seq_dict[row['item_id']] = text_to_padded_sequence(row['review'], vocab, MAX_DOC_LENGTH)\n",
    "    return user_seq_dict,item_seq_dict\n",
    "\n",
    "tr_user_seq_dict,tr_item_seq_dict = app_padding(tr_vocab)\n",
    "te_user_seq_dict,te_item_seq_dict = app_padding(te_vocab)\n",
    "print(\"Preprocessing Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1375b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "# 1. Load Pre-trained LM (Korean fastText-from Meta)\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(\"C:/Users/한승원/Downloads/cc.ko.300.vec.gz\",binary=False,limit=30000) #\"C:/Users/한승원(hanan)/Downloads/cc.ko.300.vec.gz\"\n",
    "\n",
    "EMBED_DIM = 300 \n",
    "vocab_size = len(tr_vocab) # Identical size with Vocab dict\n",
    "\n",
    "# 2. PyTorch Embedding Layer Weight: (vocab_size, EMBED_DIM)\n",
    "embedding_matrix = np.zeros((vocab_size, EMBED_DIM))\n",
    "\n",
    "# 3. 단어 사전(vocab)을 순회하며 가중치 행렬 채우기\n",
    "for word, idx in tr_vocab.items():\n",
    "    if word in word2vec_model:\n",
    "        # 사전 학습된 모델에 단어가 존재하면 해당 벡터를 가져옴\n",
    "        embedding_matrix[idx] = word2vec_model[word]\n",
    "    else:\n",
    "        # 모델에 없는 단어(OOV)는 정규분포를 따르는 랜덤 벡터로 초기화하거나 0으로 둡니다.\n",
    "        if word == '<PAD>':\n",
    "            embedding_matrix[idx] = np.zeros(EMBED_DIM)\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(EMBED_DIM,))\n",
    "\n",
    "# Numpy array -> PyTorch tensor\n",
    "embedding_tensor = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a02ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DiningCodeDataset(Dataset):\n",
    "    def __init__(self, df, user_seq_dict, item_seq_dict):\n",
    "        self.users = df['user_id'].values\n",
    "        self.items = df['item_id'].values\n",
    "        self.ratings = df['rating'].values  # Target value\n",
    "        \n",
    "        self.user_seq_dict = user_seq_dict\n",
    "        self.item_seq_dict = item_seq_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = self.users[idx]\n",
    "        item_id = self.items[idx]\n",
    "        rating = self.ratings[idx]\n",
    "\n",
    "        # 딕셔너리에서 미리 만들어둔 정수 시퀀스를 가져옴\n",
    "        u_doc = self.user_seq_dict[user_id]\n",
    "        i_doc = self.item_seq_dict[item_id]\n",
    "\n",
    "        # PyTorch 모델 입력용 텐서로 변환 (타입 변환이 매우 중요합니다)\n",
    "        u_doc_tensor = torch.tensor(u_doc, dtype=torch.long)\n",
    "        i_doc_tensor = torch.tensor(i_doc, dtype=torch.long)\n",
    "        rating_tensor = torch.tensor(rating, dtype=torch.float32)\n",
    "\n",
    "        return u_doc_tensor, i_doc_tensor, rating_tensor\n",
    "\n",
    "# DataLoader (Batch size: 64)\n",
    "BATCH_SIZE = 64\n",
    "tr_dataset = DiningCodeDataset(dc_train, tr_user_seq_dict, tr_item_seq_dict)\n",
    "tr_dataloader = DataLoader(tr_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "te_dataset = DiningCodeDataset(dc_test, te_user_seq_dict, te_item_seq_dict)\n",
    "te_dataloader = DataLoader(te_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b2f8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepCoNN(nn.Module):\n",
    "    def __init__(self,config,embedding_matrix):\n",
    "        super(DeepCoNN,self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        #1.Embedding layer\n",
    "        vocab_size,embedding_dim = embedding_matrix.shape\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(\n",
    "            torch.FloatTensor(embedding_matrix)\n",
    "        )\n",
    "        self.embedding.weight.requires_grad = True\n",
    "\n",
    "        #2-1.User Network\n",
    "        self.user_cnn = nn.Conv1d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=config.num_filters,\n",
    "            kernel_size=config.kernel_size,\n",
    "            padding=1\n",
    "        )\n",
    "        self.user_fc = nn.Linear(config.num_filters,config.latent_dim)\n",
    "\n",
    "        #2-2. Item Network\n",
    "        self.item_cnn = nn.Conv1d(\n",
    "            in_channels=embedding_dim,\n",
    "            out_channels=config.num_filters,\n",
    "            kernel_size=config.kernel_size,\n",
    "            padding=1\n",
    "        )\n",
    "        self.item_fc = nn.Linear(config.num_filters,config.latent_dim)\n",
    "\n",
    "        #Dropout\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "        #3.FM Layer (2nd-order latent vectors)\n",
    "        self.fm_linear = nn.Linear(config.latent_dim*2,1)\n",
    "        \n",
    "        self.fm_V = nn.Parameter(\n",
    "            torch.rand(config.latent_dim*2,config.fm_k)\n",
    "        )\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self,user_doc,item_doc):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            user_doc: (batch_size,max_doc_length)\n",
    "            item_doc: (batch_size,max_doc_length)\n",
    "\n",
    "        Returns:\n",
    "            rating: (batch_size,)\n",
    "        \"\"\"\n",
    "\n",
    "        #User_Net\n",
    "        user_emb = self.embedding(user_doc) # (B,L,E)\n",
    "        user_emb = user_emb.transpose(1,2) # (B,E,L) - conv1d input format\n",
    "\n",
    "        user_conv = F.relu(self.user_cnn(user_emb)) \n",
    "        user_pool = F.max_pool1d(user_conv,kernel_size=user_conv.size(2))\n",
    "        user_pool = user_pool.squeeze(2)\n",
    "\n",
    "        user_latent = F.relu(self.user_fc(user_pool))\n",
    "        user_latent = self.dropout(user_latent)\n",
    "\n",
    "        #item_Net\n",
    "        item_emb = self.embedding(item_doc) # (B,L,E)\n",
    "        item_emb = item_emb.transpose(1,2) # (B,E,L) - conv1d input format\n",
    "\n",
    "        item_conv = F.relu(self.item_cnn(item_emb)) \n",
    "        item_pool = F.max_pool1d(item_conv,kernel_size=item_conv.size(2))\n",
    "        item_pool = item_pool.squeeze(2)\n",
    "\n",
    "        item_latent = F.relu(self.item_fc(item_pool))\n",
    "        item_latent = self.dropout(item_latent)\n",
    "\n",
    "        #concatenate\n",
    "        z = torch.cat([user_latent,item_latent],dim=1)\n",
    "\n",
    "        #FM layer\n",
    "        linear_term = self.fm_linear(z)\n",
    "\n",
    "        interactions = torch.mm(z,self.fm_V)\n",
    "        interactions_squared = torch.mm(z**2,self.fm_V**2)\n",
    "\n",
    "        quadratic_term = 0.5*torch.sum(\n",
    "            interactions**2-interactions_squared,dim=1,keepdim=True\n",
    "        )\n",
    "\n",
    "        #Predict\n",
    "        rating = self.global_bias + linear_term.squeeze(1) + quadratic_term.squeeze(1)\n",
    "        \n",
    "        return rating\n",
    "\n",
    "class Config:\n",
    "\n",
    "    max_doc_length = 550\n",
    "    embedding_dim = 300\n",
    "\n",
    "    \"Model Parameters\"\n",
    "    num_filters = 100\n",
    "    kernel_size = 3\n",
    "    latent_dim = 50\n",
    "    fm_k = 8\n",
    "\n",
    "    \"Training\"\n",
    "    batch_size = 64\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.002\n",
    "    dropout_rate = 0.5\n",
    "\n",
    "    \"Other\"\n",
    "    device='cuda'\n",
    "    random_seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87084597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "config = Config()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current device: {device}\")\n",
    "model = DeepCoNN(config=config, embedding_matrix=embedding_matrix).to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(),alpha=0.9, lr=0.002, weight_decay=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915d164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/118], Loss: 14.8327\n",
      "==== Epoch 1 Complete | Avg Loss: 268.5286 ====\n",
      "\n",
      "Epoch [2/10], Step [100/118], Loss: 14.4108\n",
      "==== Epoch 2 Complete | Avg Loss: 17.6918 ====\n",
      "\n",
      "Epoch [3/10], Step [100/118], Loss: 20.8785\n",
      "==== Epoch 3 Complete | Avg Loss: 12.6092 ====\n",
      "\n",
      "Epoch [4/10], Step [100/118], Loss: 4.5436\n",
      "==== Epoch 4 Complete | Avg Loss: 7.2146 ====\n",
      "\n",
      "Epoch [5/10], Step [100/118], Loss: 2.9692\n",
      "==== Epoch 5 Complete | Avg Loss: 4.0699 ====\n",
      "\n",
      "Epoch [6/10], Step [100/118], Loss: 2.1007\n",
      "==== Epoch 6 Complete | Avg Loss: 1.9985 ====\n",
      "\n",
      "Epoch [7/10], Step [100/118], Loss: 0.9019\n",
      "==== Epoch 7 Complete | Avg Loss: 1.4352 ====\n",
      "\n",
      "Epoch [8/10], Step [100/118], Loss: 0.7938\n",
      "==== Epoch 8 Complete | Avg Loss: 1.0006 ====\n",
      "\n",
      "Epoch [9/10], Step [100/118], Loss: 0.6591\n",
      "==== Epoch 9 Complete | Avg Loss: 0.7907 ====\n",
      "\n",
      "Epoch [10/10], Step [100/118], Loss: 0.8805\n",
      "==== Epoch 10 Complete | Avg Loss: 0.7233 ====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  \n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (batch_user, batch_item, batch_rating) in enumerate(tr_dataloader):\n",
    "    \n",
    "        batch_user = batch_user.to(device)\n",
    "        batch_item = batch_item.to(device)\n",
    "        batch_rating = batch_rating.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch_user, batch_item)\n",
    "        loss = criterion(predictions, batch_rating)\n",
    "    \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Progress\n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{batch_idx+1}/{len(tr_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    avg_loss = total_loss / len(tr_dataloader)\n",
    "    print(f\"==== Epoch {epoch+1} Complete | Avg Loss: {avg_loss:.4f} ====\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d73415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dc_pred = dc_test.copy() \n",
    "pred = []\n",
    "\n",
    "with torch.no_grad():    \n",
    "    for batch_user, batch_item,_ in te_dataloader:\n",
    "        \n",
    "        batch_user = batch_user.to(device)\n",
    "        batch_item = batch_item.to(device)\n",
    "        batch_rating = batch_rating.to(device)\n",
    "        \n",
    "        batch_preds = model(batch_user,batch_item)\n",
    "        \n",
    "        pred.extend(batch_preds.cpu().numpy())\n",
    "dc_pred['rating'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af1f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.1499,0.1250,1.0000\n"
     ]
    }
   ],
   "source": [
    "dc_test.columns = ['user','item','rating','review']\n",
    "dc_pred.columns = ['user','item','rating','review']\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')  # Go up two levels to reach the directory containing 'Study'\n",
    "from Study.RecSys.matrixfactorization.preprocessing import precision_at_k,ndcg_at_k,recall_at_k\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse_par = mean_squared_error(dc_pred['rating'].values,dc_test['rating'].values,squared=False)\n",
    "\n",
    "prec_at_k_par = precision_at_k(dc_pred,dc_test,k=10)\n",
    "rec_at_k_par = recall_at_k(dc_pred,dc_test,k=10)\n",
    "#ndcg_at_k_par = ndcg_at_k(dc_pred,dc_test,k=10)\n",
    "\n",
    "print(f'RMSE: {rmse_par:.4f},Precision@K: {prec_at_k_par:.4f},Recall@K: {rec_at_k_par:.4f}') # Add NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bb7343b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dc_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a comparison dataframe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m comparison_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_rating\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mdc_pred\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactual_rating\u001b[39m\u001b[38;5;124m'\u001b[39m: dc_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m : dc_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m'\u001b[39m : dc_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m })\n\u001b[0;32m      8\u001b[0m comparison_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m     11\u001b[0m get_top_p_recommendations(comparison_df,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m먹죽귀\u001b[39m\u001b[38;5;124m'\u001b[39m,k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dc_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'predicted_rating': dc_pred['rating'].values,\n",
    "    'actual_rating': dc_test['rating'].values,\n",
    "    'user_id' : dc_test['user'],\n",
    "    'item_id' : dc_test['item']\n",
    "})\n",
    "comparison_df.head(20)\n",
    "\n",
    "\n",
    "get_top_p_recommendations(comparison_df,'먹죽귀',k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_ju",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
